PEP: 734
Title: Multiple Interpreters in the Stdlib
Author: Eric Snow <ericsnowcurrently@gmail.com>
Status: Draft
Type: Standards Track
Created: 06-Nov-2023
Python-Version: 3.13
Replaces: 554


.. note::
   This PEP is essentially a continuation of :pep:`554`.  That document
   had grown a lot of ancillary information across 7 years of discussion.
   This PEP is a reduction back to the essential information.  Much of
   that extra information is still valid and useful, just not in the
   immediate context of the specific proposal here.


Abstract
========

This PEP proposes to add a new module, ``interpreters``, to support
inspecting, creating, and running code in multiple interpreters in the
current process.  This includes ``Interpreter`` objects that represent
the underlying interpreters.  The module will also provide a basic
mechanism called "channels" for communication between interpreters.
This includes ``SendChannel`` and ``RecvChannel`` objects to represent
the pair of ends of each channel.  Finally, we will add a new
``concurrent.futures.InterpreterPoolExecutor`` based on the
``interpreters`` module.


Introduction
============

Fundamentally, an "interpreter" is the collection of (essentially)
all runtime state which Python threads must share.  So, let's first
look at threads.  Then we'll circle back to interpreters.

Threads and Thread States
-------------------------

A Python process will have one or more threads running Python code
(or otherwise interacting with the C-API).  Each of these threads
targets a distinct thread state (``PyThreadState``) which holds all
the runtime state unique to that thread.  Each thread also targets
some runtime state it shares with other threads.

A thread's distinct thread state is known as the "current" thread state.
It is stored by the runtime in a thread-local variable, and may be
looked up explicitly with ``PyThreadState_Get()``.  It gets set
automatically for the initial ("main") thread and for
``threading.Thread`` objects.  From the C-API it is set (and cleared)
by ``PyThreadState_Swap()`` and may be set by ``PyGILState_Ensure()``.
Most of the C-API requires that there be a current thread state,
either looked up implicitly or passed in as an argument.

The relationship between threads and thread states in one-to-many.
Each thread state is associated with at most a single thread and
records its thread ID.  It is never used for another thread.
In the other direction, however, a thread may have more than one thread
state associated with it, though, again, only one may be current.

When there's more than one thread state for a thread,
``PyThreadState_Swap()`` is used to switch between them,
with the requested thread state becoming the current one.
Whatever was running in the thread using the old thread state
is effectively paused until that thread state is swapped back in.

Interpreter States
------------------

As noted earlier, there is some runtime state that multiple threads
share.  Some of it is exposed by the ``sys`` module, though much is
used internally and not exposed explicitly or only through the C-API.

This state that thread states share is called the interpreter state
(``PyInterpreterState``).  We'll sometimes refer to it here as just
"interpreter", though that is also sometimes used to refer to the
``python`` executable, to the Python implementation, and to the
bytecode interpreter (i.e. ``exec()``/``eval()``).

CPython has supported multiple interpreters in the same process (AKA
"subinterpreters") since version 1.5 (1997).  The feature has been
available via the :ref:`C API <python:sub-interpreter-support>`.

Interpreters and Threads
------------------------

There's a one-to-many relationship between thread states and interpreter
states.  A thread state belongs to a single interpreter and stores
a pointer to it.  That thread state is never used for a different
interpreter.  In the other direction, however, an interpreter may have
more than thread state associated with it.  The interpreter is only
considered active in threads where one of its thread states is current.

Interpreters are created via the C-API using ``Py_NewInterpreter()``
or ``Py_NewInterpreterFromConfig()``.  Both functions do the following:

1. create a new interpreter state
2. create a new thread state
3. set the thread state as current
4. initialize the interpreter state using that thread state
5. return the thread state (still current)

To make an existing interpreter active in the current thread,
we first make sure it has a thread state for the thread.  Then
we call ``PyThreadState_Swap()`` like normal using that thread state.
If the thread state for another interpreter was already current then
it gets swapped out like normal and execution of that interpreter in
the thread is thus effectively paused until it is swapped back in.

Once an interpreter is active in the current thread like that, we can
call any of the C-API, such as ``PyEval_EvalCode()`` (i.e. ``exec()``).

The "Main" Interpreter
----------------------

When a Python process starts, it creates a single interpreter state
(the "main" interpreter) with a single thread state for the current
thread.  The Python runtime is then initialized using them.

After initialization, the script or module or REPL is executed using
them.  That execution happens in the interpreter's ``__main__`` module.

When execution is done, the Python runtime is finalized in the main
thread using the main interpreter.

Interpreter Isolation
---------------------

CPython's interpreters are intended to be strictly isolated from each
other.  That means interpreters never share objects (except in very
specific cases internally). Each interpreter has its own modules
(``sys.modules``), classes, functions, and variables.  Even where
two interpreters define the same class, each will have a distinct copy.
The same applies to state in C, including in extension modules.
The CPython C-API docs `explain more`_.

.. _explain more:
   https://docs.python.org/3/c-api/init.html#bugs-and-caveats

Notably, there is some process-global state that interpreters will
always share:

* file descriptors
* low-level env vars
* process memory (though allocators *are* isolated)
* builtin types (e.g. dict, bytes)
* singletons (e.g. None)
* underlying static module data (e.g. functions) for
  builtin/extension/frozen modules


Motivation
==========

The ``interpreters`` module will provide a high-level interface to the
multiple interpreter functionality.  The goal is to make the existing
multiple-interpreters feature of CPython more easily accessible to
Python code.  This is particularly relevant now that we have a
per-interpreter GIL (:pep:`684`) and people are more interested
in using multiple interpreters.

Without a stdlib module, users are limited to the
:ref:`C API <python:sub-interpreter-support>`, which restricts how much
they can try out and take advantage of multiple interpreters.

The module will include a basic mechanism for communicating between
interpreters.  Without one, multiple interpreters are a much less
useful feature.


Rationale
=========

A Minimal API
-------------

Since we have no experience with
how users will make use of multiple interpreters in Python code, we are
purposefully keeping the initial API as lean and minimal as possible.
The objective is to provide a well-considered foundation on which we may
add further (more advanced) functionality later.

That said, the proposed design incorporates lessons learned from
existing use of subinterpreters by the community, from existing stdlib
modules, and from other programming languages.  It also factors in
experience from using subinterpreters in the CPython test suite and
using them in `concurrency benchmarks`_.

.. _concurrency benchmarks:
   https://github.com/ericsnowcurrently/concurrency-benchmarks

Interpreter.prepare___main__() Sets Multiple Variables
------------------------------------------------------

``prepare___main__()`` may be seen as a setter function of sorts.
It supports setting multiple names at once, whereas most setters
set one item at a time.  The main reason is for efficiency.

To set a value in ``__main__.__dict__`` we must first switch to the
target interpreter, which involves some non-negligible overhead.
After setting the value we must switch back.  Furthermore, there
is some overhead to the mechanism by which we send objects between
interpreters, which can be reduced in aggregate if multiple values
are set at once.

Therefore, ``prepare___main__()`` supports setting multiple
values at once.

Propagating Exceptions
----------------------

Directly raising (a proxy of) the exception
is problematic since it's harder to distinguish between an error
in the ``Interpreter.exec()`` call and an uncaught exception
from the subinterpreter.

Channels
--------

Channels are based on `CSP`_, as is Go's concurrency model (loosely).
However, at it's core a channel is a simple FIFO queue, with the
recv and send ends exposed distinctly, so reading about CSP should
not be necessary.

.. _CSP:
   https://en.wikipedia.org/wiki/Communicating_sequential_processes

Exposing the channel ends as distinct objects, rather than combined
in one object, is a deliberate choice based on experience with Go's
channels.  Go has both directional and bidirectional channels.  The
directional channels make intentions clear and are easier to reason
about.

We cannot support directly sharing any and all objects through channels
because all objects have at least some mutable state (e.g. refcount)
which the GIL protects normally.  Furthermore, objects must only be
deleted by the interpreter that created them, which makes direct
sharing even more complex.

Supporting indirect sharing of all objects in channels would be
possible by automatically pickling them if we can't use the more
efficient mechanism.  However, it's helpful to know that only the
efficient way is being used.  Furthermore, for mutable objects
pickling would violate the guarantee that "shared" objects be
equivalent (and stay that way).  Thus the proposal does not
include automatic pickling.

Objects vs. ID Proxies
----------------------

For both interpreters and channels, we provide objects that expose them
indirectly with ID-wrapping surrogates for the underlying state.
In both cases the state is process-global and will be used by multiple
interpreters.  Thus they aren't suitable to be implemented as
``PyObject``, which is only really an option for interpreter-specific
data.  That's why the ``interpreters`` module instead provides objects
that are weakly associated through the ID.


Specification
=============

The module will:

* expose the existing multiple interpreter support
* introduce a basic mechanism for communicating between interpreters

The module will wrap a new low-level ``_interpreters`` module and
``_channels`` module (in the same way as the ``threading`` module).
However, that low-level API is not intended for public use
and thus not part of this proposal.

Using Interpreters
------------------

The module defines the following functions:

``interpreters.get_current() -> Interpreter``::

   Returns an ``Interpreter`` object for the currently executing
   interpreter.

``interpreters.list_all() -> list[Interpreter]``::

   Returns an ``Interpreter`` object for each existing interpreter,
   whether it is currently running in any threads or not.

``interpreters.create() -> Interpreter``::

   Create a new interpreter and return an ``Interpreter`` object for it.
   The interpreter will not be associated with any threads until
   something is actually run in the interpreter.

Interpreter Objects
-------------------

An ``Interpreter`` object represents the interpreter
(``PyInterpreterState``) with the corresponding ID.

If the interpreter was created with ``interpreters.create()`` then
it will be destroyed as soon as all ``Interpreter`` objects have been
deleted.

``class interpreters.Interpreter(id)``::

   An ``Interpreter`` object is not normally created directly.
   Instead, it should come from one of the module functions
   (e.g. ``create()``, ``list_all()``).

   Arguments:

   ``id`` is the ID of an existing ``PyInterpreterState`` and is used
   to target that interprter..  It must be an
   ``_interpreters.InterpreterID`` object or a non-negative ``int``.
   An ``int`` will be converted to an ``InterpreterID``.
   If an interpreter state with that ID does not exist then this raises
   ``ValueError`.

   Attibutes:

   ``id``::

      (read-only) The target interpreter's ``InterpreterID`` object.
      It is an int-like object with some internal bookkeeping.

   Methods:

   ``__hash__()``::

      Returns the hash of the interpreter's ``id``.  This is the same
      as the hash of the ID's integer value.

   ``__eq__(other)``::

      Returns ``other is self``.

   ``is_running() -> bool``::

      Returns ``True`` if the interpreter is currently executing code
      in its ``__main__`` module.  This excludes sub-threads.

      It refers only to if there is a thread
      running a script (code) in the interpreter's ``__main__`` module.
      That basically means whether or not ``Interpreter.exec()`` is
      running in some thread.  Code running in sub-threads is ignored.

   ``prepare___main__(**kwargs)``::

      Bind one or more objects in the interpreter's ``__main__`` module.

      The keyword argument names will be used as the attribute names.
      The values will be bound as new objects, though exactly equivalent
      to the original.  Only objects specifically supported for passing
      between interpreters are allowed.  See `Shareable Objects`_.

      ``prepare___main__()`` is helpful for initializing the
      globals for an interpreter before running code in it.

   ``exec(code, /)``::

      Run the given source code in the interpreter
      (in the current thread).

      This is essentially equivalent to switching to this interpreter
      in the current thread and then calling the builtin ``exec()``
      using this interpreter's ``__main__`` module's ``__dict__`` as
      the globals and locals.

      ``exec()`` does not reset the interpreter's state nor
      the ``__main__`` module, neither before nor after, so each
      successive call picks up where the last one left off.  This can
      be useful for running some code to initialize an interpreter
      (e.g. with imports) before later performing some repeated task.

Comparison with builtins.exec()
-------------------------------

``Interpreter.exec()`` is essentially the same as the builtin
``exec()``, except it targets a different interpreter, using that
interpreter's distinct runtime state.

Here are the relevant characteristics of the builtin ``exec()``,
for comparison:

* It runs in the current OS thread and pauses whatever was running there,
  which resumes when ``exec()`` finishes.  No other threads are affected.
  (To avoid pausing the current thread, run ``exec()``
  in a ``threading.Thread``.)
* It executes against a namespace, by default the ``__dict__`` of the
  current module (i.e. ``globals()``).  It uses that namespace as-is
  and does not clear it before or after.
* When code is run from the command-line (e.g. ``-m``) or the REPL,
  the "current" module is always the ``__main__`` module
  of the target (main) interpreter.
* It discards any object returned from the executed code::

   def func():
       global spam
       spam = True
       return 'a returned value'

   ns = {}
   res = exec(func.__code__, ns)
   # Nothing is returned.  The returned string was discarded.
   assert res is None, res
   assert ns['spam'] is True, ns

* It propagates any uncaught exception from the code it ran.
  The exception is raised from the ``exec()`` call in the thread
  that originally called ``exec()``.

The only difference is that, rather than propagate the uncaught
exception directly, ``Interpreter.exec()`` raises an
``interpreters.RunFailedError`` with a snapshot
(``traceback.TracebackException``) of the uncaught exception
(including its traceback) as the ``__cause__``.  That means if the
``RunFailedError`` isn't caught then the full traceback of the
propagated exception, including details about syntax errors, etc.,
will be displayed.  Having the full traceback is particularly useful
when debugging.

Communicating Between Interpreters
----------------------------------

The module introduces a basic communication mechanism called "channels".

A channel is a FIFO queue that exists outside any one interpreter.
Channels have special accommodations for safely passing object data
between interpreters, without violating interpreter isolation.
This includes thread-safety.

As with other queues in Python, each sent object is added to the back
of the channel's internal queue and each recv() pops the next one off
the front of that queue.  Every sent object will be received.

Only objects that are specifically supported for passing
between interpreters may be sent through a channel.  Note that the
actual objects aren't send, but rather their underlying data.
However, the received object will still be identical to the original.
See `Shareable Objects`_.

The module defines the following functions:

``create_channel() -> tuple[RecvChannel, SendChannel]``::

   Create a new channel, returning objects for the two ends.

Channel Objects
---------------

The ``interpreters`` module exposes channels with objects for the two
distinct ends, "send" and "recv", rather than a single object that
combines both ends (like other queues).

The receiving end of a channel:

``class RecvChannel(id)``::

   A ``RecvChannel`` object is not normally created directly.
   Instead, it should come from ``interpreters.create_channel()``.

   Arguments:

   ``id`` is a non-negative int or ``_channels.ChannelID`` corresponding
   to the ID of an existing channel and is used to target the recv end
   of that channel.  An ``int`` will be converted to a ``ChannelID``.
   If a channel that ID does not exist then this raises ``ValueError`.

   Attributes:

   ``id``::

      (read-only) The target channel's ``ChannelID`` object.
      It is an int-like object with some internal bookkeeping.

   Methods:

   ``__hash__()``::

      Returns the hash of the channel's ``id``.  This is the same
      as the hash of the ID's integer value.

   ``__eq__(other)``::

      Returns ``other is self``.

   ``recv() -> object``::

      Get the next object from the channel.  Wait if none have been sent.

   ``recv_nowait(default=None) -> object``::

      Like ``recv()``, but return the default instead of waiting.

The sending end of a channel:

``class SendChannel(id)``::

   A ``SendChannel`` object is not normally created directly.
   Instead, it should come from ``interpreters.create_channel()``.

   Arguments:

   ``id`` is a non-negative int or ``_channels.ChannelID`` corresponding
   to the ID of an existing channel and is used to target the send end
   of that channel.  An ``int`` will be converted to a ``ChannelID``.
   If a channel that ID does not exist then this raises ``ValueError`.

   Attributes:

   ``id``::

      (read-only) The target channel's ``ChannelID`` object.

   Methods:

   ``__hash__()``::

      Returns the hash of the channel's ``id``.  This is the same
      as the hash of the ID's integer value.

   ``__eq__(other)``::

      Returns ``other is self``.

   ``send(obj)``::

      Send the `shareable object <Shareable Objects_>`_ (i.e. its data)
      to the receiving end of the channel and wait for it
      to be received.

   ``send_nowait(obj) -> bool``::

      Like ``send()``, but return False if not received.

Shareable Objects
-----------------

Both ``Interpreter.prepare___main__()`` and channels work only with
"shareable" objects.

A "shareable" object is one which may be passed from one interpreter
to another.  The object is not necessarily actually shared by the
interpreters.  However, the object in the one interpreter is guaranteed
to exactly match the corresponding object in the other interpreter.

For some types (builtin singletons), the actual object is shared.
For some, the object's underlying data is actually shared but each
interpreter has a distinct object wrapping that data.  For all other
shareable types, a strict copy or proxy is made such that the
corresponding objects continue to match exactly.  In cases where
the underlying data is complex but must be copied (e.g. ``tuple``),
the data is serialized as efficiently as possible.

Shareable objects must be specifically supported internally
by the Python runtime.  However, there is no restriction against
adding support for more types later.

Here's the initial list of supported objects:

* ``str``
* ``bytes``
* ``int``
* ``float``
* ``bool`` (``True``/``False``)
* ``None``
* ``tuple`` (only with shareable items)
* channels (``SendChannel``/``RecvChannel``)
* ``memoryview`` (underlying buffer actually shared)

Note that the last two on the list, channels and ``memoryview``, are
technically mutable data types, whereas the rest are not.  When any
interpreters share mutable data there is always a risk of data races.
Cross-interpreter safety, including thread-safety, is a fundamental
feature of channels.

However, ``memoryview`` does not have any native accommodations.
The user is responsible for managing thread safety, whether passing
a token back and forth through a channel to indicate safety,
or by assigning sub-range exclusivity to individual interpreters.

Regarding channels, the primary situation for sharing is where an
interpreter is using ``prepare___main__()`` to provide the target
interpreter with their means of further communication.

Finally, a reminder: for some types the actual object is shared,
whereas for others only the underlying data (or even a copy or proxy)
is shared.  Regardless, the guarantee of "shareable" objects is that
corresponding objects in different interpreters will always strictly
match each other.

InterpreterPoolExecutor
-----------------------

The new ``concurrent.futures.InterpreterPoolExecutor`` will be
a subclass of ``concurrent.futures.ThreadPoolExecutor``, where each
worker executes tasks in its own subinterpreter.  Communication may
still be done through channels, set with the initializer.


Examples
--------

The following examples demonstrate practical cases where multiple
interpreters may be useful.

Example 1:

We have a stream of requests coming in that we will handle
via workers in sub-threads.

* each worker thread has its own interpreter
* we use one channel to send tasks to workers and
  another channel to return results
* the results are handled in a dedicated thread
* each worker keeps going until it receives a "stop" sentinel (``None``)
* the results handler keeps going until all workers have stopped

::

   import time
   import interpreters
   from mymodule import iter_requests, handle_result

   tasks_recv, tasks = interpreters.create_channel()
   results, results_send = interpreters.create_channel()

   numworkers = 20
   threads = []

   def results_handler():
       empty = object()
       running = numworkers
       while running:
           res = results.recv_nowait(empty)
           if res is empty:
               # No workers have finished a request since last time.
               time.sleep(0.1)
           elif res is None:
               # A worker has stopped.
               running -= 1
           else:
               handle_result(res)
       assert results.recv_nowait(empty) is empty
   threads.append(threading.Thread(target=results_handler))

   def worker():
       interp = interpreters.create()
       interp.prepare___main__(tasks=tasks_recv, results=results_send)
       interp.exec("""if True:
           from mymodule import handle_request, capture_exception

           while True:
               req = tasks.recv()
               if req is None:
                   # Stop!
                   break
               try:
                   res = handle_request(req)
               except Exception as exc:
                   res = capture_exception(exc)
               results.send_nowait(res)
           # Notify the results handler.
           results.send_nowait(None)
           """))
   threads.extend(threading.Thread(target=worker) for _ in range(numworkers))

   for t in threads:
       t.start()

   for req in iter_requests():
       tasks.send(req)
   # Send the "stop" signal.
   for _ in range(numworkers):
       tasks.send(None)

   for t in threads:
       t.join()

Example 2:

This case is similar to the last as we have a bunch of workers
in sub-threads.  However, this time we are chunking up a big array
of data, where each worker processes one chunk at a time.  Copying
that data to each interpreter would be exceptionally inefficient,
so we take advantage of directly sharing ``memoryview`` buffers.

* all the interpreters share the buffer of the source array
* each one writes its results to a second shared buffer
* we use a channel to send tasks to workers
* only one worker will ever read any given index in the source array
* only one worker will ever write to any given index in the results
  (this is how we ensure thread-safety)

::

   import interpreters
   import queue
   from mymodule import read_large_data_set, use_results

   numworkers = 3
   data, chunksize = read_large_data_set()
   buf = memoryview(data)
   numchunks = (len(buf) + 1) / chunksize
   results = memoryview(b'\0' * numchunks)

   tasks_recv, tasks = interpreters.create_channel()

   def worker(id):
       interp = interpreters.create()
       interp.prepare___main__(data=buf, results=results, tasks=tasks_recv)
       interp.exec("""if True:
           from mymodule import reduce_chunk

           while True:
               req = tasks.recv()
               if res is None:
                   # Stop!
                   break
               resindex, start, end = req
               chunk = data[start: end]
               res = reduce_chunk(chunk)
               results[resindex] = res
           """))
   threads = [threading.Thread(target=worker) for _ in range(numworkers)]
   for t in threads:
       t.start()

   for i in range(numchunks):
       # We assume we have at least one worker running still.
       start = i * chunksize
       end = start + chunksize
       if end > len(buf):
           end = len(buf)
       tasks.send((start, end, i))
   # Send the "stop" signal.
   for _ in range(numworkers):
       tasks.send(None)

   for t in threads:
       t.join()

   use_results(results)


Rejected Ideas
==============

See :pep:`PEP 554 <554#rejected-ideas>`.


Copyright
=========

This document is placed in the public domain or under the
CC0-1.0-Universal license, whichever is more permissive.
